from __future__ import annotations

import asyncio
import audioop
import io
import json
import re
import uuid
import wave
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

from amazon_transcribe.auth import StaticCredentialResolver
from amazon_transcribe.client import TranscribeStreamingClient

from backend.config import get_settings
from backend.services.bedrock_utils import classify_transcript_segments, _guess_category
from backend.services.s3_storage import S3Storage
from backend.utils.auth_aws import get_session
from backend.utils.time_utils import now_iso


@dataclass
class PocJob:
    job_id: str
    agenda_text: str
    audio_filename: str
    created_at: str = field(default_factory=now_iso)
    status: str = "processing"
    transcripts: list[dict[str, Any]] = field(default_factory=list)
    queue: asyncio.Queue = field(default_factory=asyncio.Queue)
    speaker_labels: dict[str, str] = field(default_factory=dict)
    next_speaker_index: int = 1
    next_entry_index: int = 1
    pending_results: dict[str, dict[str, Any]] = field(default_factory=dict)
    processed_result_ids: set[str] = field(default_factory=set)
    classified_segments: list[dict[str, Any]] = field(default_factory=list)


class POCController:
    def __init__(self, storage_dir: Path | None = None):
        self.storage_dir = storage_dir or Path(__file__).resolve().parents[1] / "data" / "poc"
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.jobs: dict[str, PocJob] = {}
        self.settings = get_settings()
        self.logger = logging.getLogger(__name__)
        self.archive_storage = S3Storage(bucket="meetingpolice-test")

    async def start_transcription(self, agenda_text: str, audio_filename: str, audio_bytes: bytes) -> str:
        job_id = uuid.uuid4().hex[:12]
        job = PocJob(job_id=job_id, agenda_text=agenda_text, audio_filename=audio_filename)
        self.jobs[job_id] = job

        job_dir = self._job_dir(job_id)
        job_dir.mkdir(parents=True, exist_ok=True)
        (job_dir / "agenda.txt").write_text(agenda_text, encoding="utf-8")
        (job_dir / "audio.bin").write_bytes(audio_bytes)

        asyncio.create_task(self._process_audio(job, audio_bytes))
        return job_id

    def get_job(self, job_id: str) -> PocJob | None:
        return self.jobs.get(job_id)

    def get_job_payload(self, job_id: str) -> dict[str, Any]:
        job = self.get_job(job_id)
        if not job:
            raise KeyError(job_id)
        return {
            "job_id": job.job_id,
            "status": job.status,
            "agenda_text": job.agenda_text,
            "audio_filename": job.audio_filename,
            "created_at": job.created_at,
            "transcripts": job.transcripts,
            "classified_segments": job.classified_segments,
        }

    #æœ€çµ‚çµæœãŒå‡ºãŸã‚‰åˆ†æé–‹å§‹ã™ã‚‹
    async def classify_realtime(self, job_id: str, text: str, speaker: str, index: int) -> dict[str, Any]:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§1ã¤ã®ç™ºè¨€ã‚’ç°¡æ˜“åˆ†æï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ–¹å¼ï¼‰"""
        # 10æ–‡å­—æœªæº€ã®çŸ­ã„ç™ºè¨€ã¯åˆ†æã—ãªã„
        text_stripped = text.strip()
        print(f"ğŸ” æ–‡å­—æ•°ãƒã‚§ãƒƒã‚¯: '{text_stripped}' â†’ len={len(text_stripped)}")
        if len(text_stripped) < 10:
            print(f"  âœ‹ 10æ–‡å­—æœªæº€ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼")
            return {}
        
        print(f"ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æé–‹å§‹: {speaker} - {text[:30]}...")
        self.logger.info(f"ğŸ“Š ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æé–‹å§‹: {speaker} - {text[:30]}...")
        job = self.get_job(job_id)
        if not job:
            raise KeyError(job_id)
        
        # ãƒ¡ã‚¿æƒ…å ±ï¼ˆè­°é¡Œã€æ‰€è¦æ™‚é–“ã€ç™ºè¡¨è€…ãªã©ï¼‰ã®ç™ºè¨€ã¯ã‚¹ã‚­ãƒƒãƒ—
        # ã€ŒAgenda topic:ã€ã§å§‹ã¾ã‚‹ç™ºè¨€ã¯å…¨ã¦ã‚¹ã‚­ãƒƒãƒ—
        if text.startswith("Agenda topic:"):
            print(f"  â†’ ãƒ¡ã‚¿æƒ…å ±ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {text[:30]}...")
            return {}
        
        # ã€ŒDiscussion:ã€ã®å¾Œã«ç¶šãå†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
        if text.startswith("Discussion: Confirming action items for"):
            # ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ã‚’æŠ½å‡º
            import re
            match = re.search(r"'([^']+)'", text)
            if match:
                content = match.group(1)
                # ãƒ¡ã‚¿æƒ…å ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                meta_keywords = ["è­°é¡Œã‚¿ã‚¤ãƒˆãƒ«", "æ‰€è¦æ™‚é–“", "ç™ºè¡¨è€…", "åˆ†", "æ™‚é–“"]
                # çŸ­ã„å˜èªï¼ˆ10æ–‡å­—æœªæº€ï¼‰ã§ã€ãƒ¡ã‚¿æƒ…å ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if len(content) < 10 or any(keyword in content for keyword in meta_keywords):
                    print(f"  â†’ ãƒ¡ã‚¿æƒ…å ±ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {text[:50]}...")
                    return {}
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“åˆ†é¡ï¼ˆå³åº§ã«è¿”ã™ï¼‰
        category_quick = _guess_category(text)
        alignment_quick = self._calculate_alignment(text, job.agenda_text)
        
        result_quick = {
            "index": index,
            "text": text,
            "speaker": speaker,
            "category": category_quick,
            "alignment": alignment_quick,
            "method": "keyword",  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹
            "is_final": False  # ã¾ã ç¢ºå®šã˜ã‚ƒãªã„
        }
        
        # ã™ãã«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€šçŸ¥
        await job.queue.put({"type": "realtime_classification", "payload": result_quick})
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§Bedrockã«é€ä¿¡ï¼ˆéåŒæœŸï¼‰
        asyncio.create_task(self._classify_with_bedrock(job, text, speaker, index))
        
        return result_quick

    def list_archived_jobs(self, limit: int = 20) -> list[dict[str, Any]]:
        keys = [key for key in self.archive_storage.list_objects("poc/") if key.endswith(".json")]
        items: list[dict[str, Any]] = []
        for key in sorted(keys, reverse=True):
            try:
                data = self._load_archived_job(key)
            except (FileNotFoundError, json.JSONDecodeError):
                continue
            items.append(
                {
                    "job_id": data.get("job_id"),
                    "completed_at": data.get("completed_at"),
                    "archive_name": data.get("archive_name") or "",
                    "agenda_preview": (data.get("agenda_text") or "")[:80],
                    "transcript_count": len(data.get("transcripts") or []),
                }
            )
            if len(items) >= limit:
                break
        return items

    def get_archived_job(self, job_id: str) -> dict[str, Any]:
        data = self._load_archived_job(self._archive_key(job_id))
        if not data:
            raise KeyError(job_id)
        return data



    async def _process_audio(self, job: PocJob, audio_bytes: bytes) -> None:
        try:
            pcm_bytes, sample_rate = self._prepare_pcm(audio_bytes)
            await self._run_transcribe_stream(job, pcm_bytes, sample_rate)
        except Exception:
            self.logger.exception("Transcribe streaming failed for job %s, fallback to mock data", job.job_id)
            job.transcripts.clear()
            await self._simulate_stream(job)

    async def _simulate_stream(self, job: PocJob) -> None:
        script = self._build_script(job)
        job_dir = self._job_dir(job.job_id)
        transcript_path = job_dir / "transcripts.json"
        for idx, line in enumerate(script, start=1):
            payload = {
                "index": idx,
                "speaker": "Speaker A" if idx % 2 else "Speaker B",
                "raw_speaker": "spk_mock_a" if idx % 2 else "spk_mock_b",
                "result_id": f"mock-{idx}",
                "text": line,
                "timestamp": now_iso(),
            }
            job.transcripts.append(payload)
            job.next_entry_index = max(job.next_entry_index, idx + 1)
            await job.queue.put({"type": "transcript", "action": "append", "payload": payload})
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚’å®Ÿè¡Œ
            asyncio.create_task(self.classify_realtime(job.job_id, line, payload["speaker"], idx))
            await asyncio.sleep(1.2)
        
        # å…¨ã¦ã®åˆ†æãŒå®Œäº†ã™ã‚‹ã¾ã§å°‘ã—å¾…ã¤
        await asyncio.sleep(3)
        
        job.status = "completed"
        transcript_path.write_text(json.dumps(job.transcripts, ensure_ascii=False, indent=2), encoding="utf-8")
        self._persist_transcripts(job)
        await job.queue.put({"type": "complete"})

    def _build_script(self, job: PocJob) -> list[str]:
        agenda_lines = [
            line.strip(" -*â€¢\t")
            for line in job.agenda_text.splitlines()
            if line.strip(" -*â€¢\t")
        ]
        script: list[str] = []
        
        # ãƒªã‚¢ãƒ«ãªä¼šè­°ã®ç™ºè¨€ã‚’ç”Ÿæˆ
        if agenda_lines:
            # ãƒ¡ã‚¿æƒ…å ±ã®è¡Œï¼ˆè­°é¡Œã‚¿ã‚¤ãƒˆãƒ«ã€ç™ºè¡¨è€…ã€æ‰€è¦æ™‚é–“ãªã©ï¼‰
            for line in agenda_lines:
                script.append(f"Agenda topic: {line}")
            
            # å®Ÿéš›ã®è­°è«–å†…å®¹ã‚’è¿½åŠ 
            script.append("ç¾åœ¨ã®é›¢è„±ç‡ã¯ç´„30%ã§ã€ç‰¹ã«ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ç™»éŒ²ç”»é¢ã§å¤šãç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚")
            script.append("ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆã®çµæœã€å…¥åŠ›é …ç›®ãŒå¤šã™ãã‚‹ã¨ã„ã†æ„è¦‹ãŒå¤šã‹ã£ãŸã§ã™ã€‚")
            script.append("å…¥åŠ›é …ç›®ã‚’å¿…é ˆé …ç›®ã ã‘ã«çµã‚‹ã“ã¨ã‚’ææ¡ˆã—ã¾ã™ã€‚")
            script.append("ãã‚Œã¯è‰¯ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã§ã™ã­ã€‚å…·ä½“çš„ã«ã©ã®é …ç›®ã‚’æ®‹ã—ã¾ã™ã‹ï¼Ÿ")
            script.append("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã ã‘ã«ã—ã¦ã€ä»–ã¯å¾Œã‹ã‚‰å…¥åŠ›ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚")
            script.append("äº†è§£ã—ã¾ã—ãŸã€‚ãã®æ–¹å‘ã§å®Ÿè£…ã‚’é€²ã‚ã¾ã—ã‚‡ã†ã€‚")
            script.append("å®Ÿè£…æœŸé–“ã¯ã©ã®ãã‚‰ã„ã‚’è¦‹è¾¼ã‚“ã§ã„ã¾ã™ã‹ï¼Ÿ")
            script.append("2é€±é–“ç¨‹åº¦ã§å®Œäº†ã§ãã‚‹ã¨æ€ã„ã¾ã™ã€‚")
        
        if not script:
            script.append(f"Processing uploaded audio file '{job.audio_filename}'")
            script.append("Let's discuss the main topics for today's meeting.")
            script.append("I agree, we should focus on the key action items.")

        return script[:15]  # æœ€å¤§15ä»¶

    def _job_dir(self, job_id: str) -> Path:
        return self.storage_dir / job_id

    def _prepare_pcm(self, audio_bytes: bytes) -> tuple[bytes, int]:
        try:
            with wave.open(io.BytesIO(audio_bytes), "rb") as wav:
                sample_width = wav.getsampwidth()
                sample_rate = wav.getframerate()
                channels = wav.getnchannels()
                raw = wav.readframes(wav.getnframes())
        except wave.Error:
            # assume already PCM (e.g. raw upload)
            return audio_bytes, 16000

        target_width = 2
        if sample_width != target_width:
            raw = audioop.lin2lin(raw, sample_width, target_width)
            sample_width = target_width

        if channels != 1:
            raw = audioop.tomono(raw, sample_width, 0.5, 0.5)
            channels = 1

        target_rate = 16000
        if sample_rate != target_rate:
            raw, _ = audioop.ratecv(raw, sample_width, channels, sample_rate, target_rate, None)
            sample_rate = target_rate

        return raw, sample_rate

    async def _run_transcribe_stream(self, job: PocJob, pcm_bytes: bytes, sample_rate: int) -> None:
        session = get_session()
        credentials = session.get_credentials()
        if not credentials:
            raise RuntimeError("Unable to resolve AWS credentials for Transcribe streaming")
        frozen = credentials.get_frozen_credentials()
        credential_resolver = StaticCredentialResolver(
            frozen.access_key,
            frozen.secret_key,
            frozen.token,
        )
        client = TranscribeStreamingClient(
            region=self.settings.aws_region,
            credential_resolver=credential_resolver,
        )
        chunk_ms = 50
        chunk_bytes = max(1, int(sample_rate * 2 * chunk_ms / 1000))

        self.logger.info(
            "Starting Transcribe stream job_id=%s sample_rate=%s chunk_bytes=%s",
            job.job_id,
            sample_rate,
            chunk_bytes,
        )
        stream = await client.start_stream_transcription(
            language_code="ja-JP",
            media_encoding="pcm",
            media_sample_rate_hz=sample_rate,
            show_speaker_label=True,
            enable_partial_results_stabilization=True,
            partial_results_stability="medium",
        )

        async def send_audio():
            chunk_delay = chunk_ms / 1000
            for chunk in self._chunk_pcm(pcm_bytes, chunk_bytes):
                await stream.input_stream.send_audio_event(audio_chunk=chunk)
                await asyncio.sleep(chunk_delay)
            await stream.input_stream.end_stream()

        async def consume_results():
            async for event in stream.output_stream:
                transcript = getattr(event, "transcript", None)
                if not transcript:
                    continue
                for result in getattr(transcript, "results", []) or []:
                    result_id = getattr(result, "result_id", None)
                    if not result_id:
                        continue
                    is_partial = getattr(result, "is_partial", False)
                    if not is_partial and result_id in job.processed_result_ids:
                        continue
                    alternatives = getattr(result, "alternatives", []) or []
                    if not alternatives:
                        continue
                    alternative = alternatives[0]
                    text = (getattr(alternative, "transcript", "") or "").strip()
                    if not text:
                        continue
                    speaker_label, raw_label = self._speaker_from_items(job, alternative)
                    await self._handle_result(job, result_id, speaker_label, raw_label, text, not is_partial)
                    if not is_partial:
                        job.processed_result_ids.add(result_id)

        success = False
        try:
            await asyncio.gather(send_audio(), consume_results())
            success = True
        finally:
            await self._finalize_pending_results(job)
            if success:
                job.status = "completed"
                self._persist_transcripts(job)
                await job.queue.put({"type": "complete"})
                self.logger.info("Transcribe stream completed job_id=%s total_segments=%s", job.job_id, len(job.transcripts))

    def _chunk_pcm(self, pcm_bytes: bytes, chunk_size: int):
        for idx in range(0, len(pcm_bytes), chunk_size):
            yield pcm_bytes[idx : idx + chunk_size]

    def _speaker_name(self, job: PocJob, raw_label: str | None) -> str:
        key = raw_label or "__unknown__"
        if key not in job.speaker_labels:
            label = f"Speaker {job.next_speaker_index}"
            job.speaker_labels[key] = label
            job.next_speaker_index += 1
        return job.speaker_labels[key]

    def _speaker_from_items(self, job: PocJob, alternative: Any) -> tuple[str, str]:
        counts: dict[str, int] = {}
        for item in getattr(alternative, "items", []) or []:
            label = getattr(item, "speaker", None)
            if not label:
                continue
            counts[label] = counts.get(label, 0) + 1
        raw_label = max(counts, key=counts.get) if counts else None
        friendly = self._speaker_name(job, raw_label)
        return friendly, (raw_label or "spk_unk")

    async def _handle_result(self, job: PocJob, result_id: str, speaker_label: str, raw_label: str, text: str, is_final: bool) -> None:
        entry = job.pending_results.get(result_id)
        if not entry:
            entry = {
                "index": job.next_entry_index,
                "speaker": speaker_label,
                "raw_speaker": raw_label,
                "result_id": result_id,
                "text": text,
                "timestamp": now_iso(),
            }
            job.next_entry_index += 1
            job.pending_results[result_id] = entry
            await job.queue.put({"type": "transcript", "action": "append", "payload": self._public_payload(entry)})
        else:
            if entry["text"] == text and entry["speaker"] == speaker_label:
                if is_final:
                    await self._finalize_result(job, result_id)
                    # æœ€çµ‚çµæœãŒå‡ºãŸã‚‰ã€ã™ãã«åˆ†æé–‹å§‹
                    asyncio.create_task(self.classify_realtime(job.job_id, text, speaker_label, entry["index"]))
                return
            entry["text"] = text
            entry["speaker"] = speaker_label
            entry["raw_speaker"] = raw_label
            await job.queue.put({"type": "transcript", "action": "update", "payload": self._public_payload(entry)})
        
        # æœ€çµ‚çµæœãŒå‡ºãŸã‚‰ã€å¿…ãšåˆ†æã‚’å®Ÿè¡Œ
        if is_final:
            await self._finalize_result(job, result_id)
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚’é–‹å§‹
            asyncio.create_task(self.classify_realtime(job.job_id, entry["text"], entry["speaker"], entry["index"]))

    async def _finalize_result(self, job: PocJob, result_id: str) -> None:
        entry = job.pending_results.pop(result_id, None)
        if not entry:
            return
        payload = self._public_payload(entry)
        job.transcripts.append(payload)
        await job.queue.put({"type": "transcript", "action": "update", "payload": payload})

    async def _finalize_pending_results(self, job: PocJob) -> None:
        for result_id in list(job.pending_results.keys()):
            await self._finalize_result(job, result_id)

    def _public_payload(self, entry: dict[str, Any]) -> dict[str, Any]:
        return {
            "index": entry["index"],
            "speaker": entry["speaker"],
            "raw_speaker": entry.get("raw_speaker", entry["speaker"]),
            "result_id": entry.get("result_id"),
            "text": entry["text"],
            "timestamp": entry["timestamp"],
        }



    def _persist_transcripts(self, job: PocJob) -> None:
        try:
            archive_name = self._suggest_archive_slug(job)
            payload = {
                "job_id": job.job_id,
                "agenda_text": job.agenda_text,
                "completed_at": now_iso(),
                "transcripts": job.transcripts,
                "archive_name": archive_name,
            }
            key = self._build_archive_key(job.job_id, archive_name)
            self.archive_storage.write_json(key, payload)
        except Exception:
            self.logger.exception("Failed to archive transcripts for job %s", job.job_id)

    def _load_archived_job(self, key: str) -> dict[str, Any]:
        raw = self.archive_storage.read_text(key)
        return json.loads(raw)

    def _archive_key(self, job_id: str) -> str:
        suffix = f"{job_id}.json"
        for key in self.archive_storage.list_objects("poc/"):
            if key.endswith(suffix):
                return key
        return f"poc/{job_id}.json"

    def _build_archive_key(self, job_id: str, archive_name: str | None) -> str:
        slug = archive_name or ""
        if slug:
            slug = slug[:40]
            return f"poc/{slug}-{job_id}.json"
        return f"poc/{job_id}.json"

    def _suggest_archive_slug(self, job: PocJob) -> str:
        source = job.agenda_text or ""
        if not source.strip():
            for transcript in job.transcripts:
                text = transcript.get("text", "")
                if text.strip():
                    source = text
                    break
        if not source.strip():
            return ""
        first_line = source.splitlines()[0].strip()
        cleaned = re.sub(r"[\sã€€]+", "-", first_line)
        cleaned = re.sub(r"[^0-9A-Za-zã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ãƒ¼_-]", "", cleaned)
        cleaned = cleaned.strip("-_")
        return cleaned[:40]




    async def _classify_with_bedrock(self, job: PocJob, text: str, speaker: str, index: int) -> None:
        """Bedrockã§é«˜ç²¾åº¦ãªåˆ†æã‚’å®Ÿè¡Œï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰"""
        print(f"ğŸ” Bedrockåˆ†æé–‹å§‹: {speaker} - {text[:30]}...")
        self.logger.info(f"ğŸ” Bedrockåˆ†æé–‹å§‹: {speaker} - {text[:30]}...")
        try:
            # æ–‡è„ˆã‚’å–å¾—ï¼ˆå‰å¾Œã®ç™ºè¨€ï¼‰
            context_before = ""
            context_after = ""
            for transcript in job.transcripts:
                if transcript.get("index") == index - 1:
                    context_before = transcript.get("text", "")
                elif transcript.get("index") == index + 1:
                    context_after = transcript.get("text", "")
            
            # Bedrockã§åˆ†æ
            segment = {
                "index": index,
                "speaker": speaker,
                "text": text,
                "context_before": context_before,
                "context_after": context_after,
            }
            
            # classify_transcript_segmentsã‚’ä½¿ã£ã¦åˆ†æ
            print(f"  â†’ Bedrockã«é€ä¿¡ä¸­... segment={segment}")
            print(f"  â†’ agenda_text={job.agenda_text[:100]}...")
            classified = await asyncio.to_thread(
                classify_transcript_segments,
                [segment],
                job.agenda_text
            )
            print(f"  â†’ Bedrockã‹ã‚‰å¿œç­”å—ä¿¡: {classified}")
            print(f"  â†’ classified type: {type(classified)}, len: {len(classified) if isinstance(classified, list) else 'N/A'}")
            
            if classified and len(classified) > 0:
                result = classified[0]
                category_ai = result.get("category", _guess_category(text))
                alignment_ai = result.get("alignment", 0)
                
                result_ai = {
                    "index": index,
                    "text": text,
                    "speaker": speaker,
                    "category": category_ai,
                    "alignment": alignment_ai,
                    "method": "bedrock",  # AIåˆ†æ
                    "is_final": True  # ç¢ºå®š
                }
                
                # æ›´æ–°ã‚’ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«é€šçŸ¥
                await job.queue.put({"type": "realtime_classification", "action": "update", "payload": result_ai})
                
                print(f"âœ… Bedrockåˆ†æå®Œäº†: {speaker} - {text[:30]}... â†’ [{category_ai}] {alignment_ai}%")
                self.logger.info(f"Bedrockåˆ†æå®Œäº†: {speaker} - {text} â†’ [{category_ai}] {alignment_ai}%")
            else:
                # BedrockãŒå¤±æ•—ã—ãŸã‚‰ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®çµæœã‚’ã€Œç¢ºå®šã€ã¨ã—ã¦é€ã‚‹
                print(f"âš ï¸ Bedrockã‹ã‚‰çµæœãªã—ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’ç¢ºå®šã¨ã—ã¦é€ä¿¡")
                category_fallback = _guess_category(text)
                alignment_fallback = self._calculate_alignment(text, job.agenda_text)
                
                result_fallback = {
                    "index": index,
                    "text": text,
                    "speaker": speaker,
                    "category": category_fallback,
                    "alignment": alignment_fallback,
                    "method": "keyword",  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹
                    "is_final": True  # ç¢ºå®šï¼ˆBedrockãŒå¤±æ•—ã—ãŸã®ã§ï¼‰
                }
                
                await job.queue.put({"type": "realtime_classification", "action": "update", "payload": result_fallback})
                print(f"âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æç¢ºå®š: {speaker} - {text[:30]}... â†’ [{category_fallback}] {alignment_fallback}%")
        
        except Exception as e:
            print(f"âŒ Bedrockåˆ†æå¤±æ•—: {e}")
            print(f"è©³ç´°: {type(e).__name__}: {str(e)}")
            self.logger.error(f"âŒ Bedrockåˆ†æå¤±æ•—: {e}")
            self.logger.exception("è©³ç´°ãªã‚¨ãƒ©ãƒ¼:")
            # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®çµæœã¯æ®‹ã‚‹ã®ã§å•é¡Œãªã—

    def _calculate_alignment(self, text: str, agenda_text: str) -> int:
        """ç™ºè¨€ã¨ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã®ä¸€è‡´åº¦ã‚’0-100ã§è¨ˆç®—ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰"""
        if not agenda_text or not agenda_text.strip():
            return 50  # ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ãŒãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ50%
        
        # ãƒ¡ã‚¿æƒ…å ±ï¼ˆè­°é¡Œã€æ‰€è¦æ™‚é–“ã€ç™ºè¡¨è€…ãªã©ï¼‰ã®ç™ºè¨€ã¯ä¸€è‡´åº¦ã‚’è¨ˆç®—ã—ãªã„
        meta_keywords = ["è­°é¡Œ", "ã‚¿ã‚¤ãƒˆãƒ«", "æ‰€è¦æ™‚é–“", "ç™ºè¡¨è€…", "Agenda topic:", "åˆ†", "æ™‚é–“"]
        if any(keyword in text for keyword in meta_keywords):
            return 50  # ãƒ¡ã‚¿æƒ…å ±ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ50%
        
        # ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆãƒ¡ã‚¿æƒ…å ±ã‚’é™¤å¤–ï¼‰
        agenda_keywords = set()
        skip_keywords = {"è­°é¡Œ", "ã‚¿ã‚¤ãƒˆãƒ«", "æ‰€è¦æ™‚é–“", "ç™ºè¡¨è€…", "æ¤œè¨äº‹é …", "ç›®çš„", "èƒŒæ™¯"}
        
        for line in agenda_text.splitlines():
            line = line.strip(" -*â€¢\t0123456789.ã€‚")  # ç®‡æ¡æ›¸ãè¨˜å·ã‚„ç•ªå·ã‚’é™¤å»
            if not line:
                continue
            # ãƒ¡ã‚¿æƒ…å ±ã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if any(skip in line for skip in skip_keywords):
                continue
            # 2æ–‡å­—ä»¥ä¸Šã®å˜èªã‚’æŠ½å‡ºï¼ˆç°¡æ˜“çš„ï¼‰
            words = [w for w in re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾ ãƒ¼]+', line) if len(w) >= 2]
            # ã‚¹ã‚­ãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é™¤å¤–
            words = [w for w in words if w not in skip_keywords]
            agenda_keywords.update(words)
        
        if not agenda_keywords:
            return 50  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ50%
        
        # ç™ºè¨€ã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        text_lower = text.lower()
        matched_count = sum(1 for keyword in agenda_keywords if keyword in text_lower)
        
        # ä¸€è‡´ç‡ã‚’è¨ˆç®—ï¼ˆ0-100%ï¼‰
        if matched_count == 0:
            return 30  # å…¨ãä¸€è‡´ã—ãªãã¦ã‚‚æœ€ä½30%
        
        # ãƒãƒƒãƒç‡ã«åŸºã¥ã„ã¦è¨ˆç®—ï¼ˆã‚ˆã‚Šå¯›å®¹ã«ï¼‰
        match_ratio = matched_count / len(agenda_keywords)
        alignment = min(100, int(30 + (match_ratio * 70)))  # 30%ã€œ100%ã®ç¯„å›²
        
        return alignment
